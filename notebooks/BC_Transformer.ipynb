{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # for loading json file\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import gymnasium as gym\n",
    "import mani_skill2.envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from mani_skill2.utils.wrappers import RecordEpisode\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.dataset import StackDatasetOriginalSequential\n",
    "from utils.data_utils import flatten_obs, make_path\n",
    "from utils.train_utils import init_deque, update_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ckpt_path = make_path('BC_LSTM', 'checkpoints')\n",
    "log_path = make_path('BC_LSTM', 'logs')\n",
    "tensorboard_path = make_path('BC_LSTM', 'logs', 'tensorboard')\n",
    "\n",
    "Path(ckpt_path).mkdir(exist_ok=True, parents=True)\n",
    "Path(log_path).mkdir(exist_ok=True, parents=True)\n",
    "Path(tensorboard_path).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8444e-02, -3.2075e-02,  4.4568e-02,  8.4997e-03, -3.8891e-02,\n",
       "         -7.3362e-02,  9.5360e-03, -1.8447e-03],\n",
       "        [-5.8006e-02, -3.2533e-02,  4.3997e-02,  8.7425e-03, -3.8996e-02,\n",
       "         -7.3627e-02,  8.0062e-03, -3.7727e-03],\n",
       "        [-5.8667e-02, -2.9730e-02,  4.3368e-02,  7.6694e-03, -3.9701e-02,\n",
       "         -7.5524e-02,  1.2014e-02, -2.7713e-03],\n",
       "        [-5.9446e-02, -3.2544e-02,  4.4476e-02,  6.3791e-03, -3.7693e-02,\n",
       "         -7.3764e-02,  1.0237e-02, -1.1036e-03],\n",
       "        [-5.7198e-02, -3.1380e-02,  4.3718e-02,  5.8235e-03, -3.9074e-02,\n",
       "         -7.4221e-02,  9.8430e-03, -1.2061e-03],\n",
       "        [-6.0016e-02, -3.0119e-02,  4.4182e-02,  7.4890e-03, -3.8501e-02,\n",
       "         -7.6406e-02,  1.2176e-02, -1.3518e-03],\n",
       "        [-5.8501e-02, -3.1601e-02,  4.2506e-02,  9.0375e-03, -3.9788e-02,\n",
       "         -7.4813e-02,  9.1560e-03, -2.8596e-03],\n",
       "        [-5.8827e-02, -3.1592e-02,  4.4111e-02,  7.8881e-03, -3.7627e-02,\n",
       "         -7.3742e-02,  9.8016e-03, -6.4881e-04],\n",
       "        [-5.7797e-02, -3.2534e-02,  4.2962e-02,  6.6346e-03, -3.8238e-02,\n",
       "         -7.3301e-02,  9.0640e-03, -2.7073e-03],\n",
       "        [-5.7899e-02, -3.2162e-02,  4.4968e-02,  6.8399e-03, -3.8530e-02,\n",
       "         -7.5440e-02,  9.0220e-03, -1.8434e-03],\n",
       "        [-5.7808e-02, -3.1471e-02,  4.3754e-02,  1.0152e-02, -4.0036e-02,\n",
       "         -7.5980e-02,  1.0148e-02, -2.1922e-03],\n",
       "        [-5.9281e-02, -3.1794e-02,  4.3772e-02,  8.5250e-03, -3.9168e-02,\n",
       "         -7.4514e-02,  1.1430e-02, -3.1830e-04],\n",
       "        [-5.8084e-02, -3.0099e-02,  4.2847e-02,  9.0234e-03, -4.0163e-02,\n",
       "         -7.4973e-02,  9.5721e-03, -2.1931e-03],\n",
       "        [-5.8645e-02, -3.1752e-02,  4.4883e-02,  8.7386e-03, -3.6526e-02,\n",
       "         -7.6137e-02,  1.2211e-02, -6.0627e-04],\n",
       "        [-5.6911e-02, -3.2094e-02,  4.3082e-02,  7.0215e-03, -3.9930e-02,\n",
       "         -7.5659e-02,  8.5985e-03, -3.5399e-03],\n",
       "        [-5.6871e-02, -3.0363e-02,  4.2800e-02,  6.2052e-03, -4.0088e-02,\n",
       "         -7.4765e-02,  1.0613e-02, -2.7798e-03],\n",
       "        [-5.7814e-02, -3.2481e-02,  4.4381e-02,  8.2899e-03, -3.8477e-02,\n",
       "         -7.5372e-02,  9.6390e-03, -1.8860e-03],\n",
       "        [-5.8625e-02, -3.2195e-02,  4.4676e-02,  7.6050e-03, -3.8191e-02,\n",
       "         -7.5099e-02,  8.9685e-03,  1.9834e-04],\n",
       "        [-5.9948e-02, -3.1704e-02,  4.3929e-02,  6.3991e-03, -3.8272e-02,\n",
       "         -7.5095e-02,  1.0333e-02, -1.9971e-03],\n",
       "        [-5.9107e-02, -3.1624e-02,  4.3103e-02,  8.2200e-03, -3.8138e-02,\n",
       "         -7.5017e-02,  9.1235e-03, -1.7409e-03],\n",
       "        [-5.7900e-02, -3.0933e-02,  4.2439e-02,  9.1946e-03, -4.1071e-02,\n",
       "         -7.5611e-02,  1.2670e-02, -1.9376e-03],\n",
       "        [-5.8103e-02, -3.2345e-02,  4.3700e-02,  7.6713e-03, -3.8110e-02,\n",
       "         -7.3876e-02,  1.1341e-02, -1.5983e-03],\n",
       "        [-5.9620e-02, -2.9707e-02,  4.2978e-02,  9.0058e-03, -3.8419e-02,\n",
       "         -7.3693e-02,  1.0432e-02, -1.7338e-03],\n",
       "        [-5.8979e-02, -3.1580e-02,  4.3458e-02,  8.4422e-03, -3.9607e-02,\n",
       "         -7.6692e-02,  1.1912e-02, -6.0893e-04],\n",
       "        [-5.9649e-02, -3.3098e-02,  4.4488e-02,  8.2130e-03, -3.8192e-02,\n",
       "         -7.4797e-02,  9.3707e-03, -3.5129e-04],\n",
       "        [-5.9370e-02, -2.9985e-02,  4.3988e-02,  7.9728e-03, -3.9057e-02,\n",
       "         -7.3994e-02,  1.0482e-02, -8.1032e-04],\n",
       "        [-5.8681e-02, -3.1072e-02,  4.3765e-02,  9.0619e-03, -3.7946e-02,\n",
       "         -7.4300e-02,  9.3801e-03, -1.0198e-03],\n",
       "        [-5.8965e-02, -3.3445e-02,  4.3942e-02,  7.7736e-03, -3.6931e-02,\n",
       "         -7.3710e-02,  9.8343e-03, -1.8700e-03],\n",
       "        [-5.8425e-02, -3.1963e-02,  4.1746e-02,  7.7642e-03, -3.9728e-02,\n",
       "         -7.5169e-02,  1.2502e-02, -1.2495e-03],\n",
       "        [-5.8749e-02, -3.0127e-02,  4.2673e-02,  7.7937e-03, -3.9463e-02,\n",
       "         -7.6762e-02,  1.0020e-02, -1.4461e-03],\n",
       "        [-5.8066e-02, -3.1385e-02,  4.3874e-02,  7.7487e-03, -3.9470e-02,\n",
       "         -7.4476e-02,  1.1080e-02, -8.0772e-04],\n",
       "        [-5.8587e-02, -3.0569e-02,  4.2998e-02,  6.8897e-03, -3.8172e-02,\n",
       "         -7.4832e-02,  1.1173e-02, -3.5992e-03],\n",
       "        [-5.6250e-02, -3.0940e-02,  4.4033e-02,  7.3795e-03, -3.9798e-02,\n",
       "         -7.5446e-02,  1.1475e-02, -3.1342e-03],\n",
       "        [-5.8037e-02, -3.2175e-02,  4.4170e-02,  6.6416e-03, -3.8089e-02,\n",
       "         -7.5072e-02,  1.0289e-02, -1.5436e-03],\n",
       "        [-5.7090e-02, -3.2431e-02,  4.3024e-02,  8.1160e-03, -3.9195e-02,\n",
       "         -7.4220e-02,  1.0961e-02, -2.2886e-03],\n",
       "        [-5.8879e-02, -3.0349e-02,  4.1996e-02,  9.5450e-03, -3.9676e-02,\n",
       "         -7.6616e-02,  1.0148e-02, -1.7456e-03],\n",
       "        [-5.7320e-02, -3.0804e-02,  4.2694e-02,  8.6959e-03, -3.9080e-02,\n",
       "         -7.4990e-02,  1.1220e-02, -2.6728e-03],\n",
       "        [-5.7966e-02, -3.1204e-02,  4.4564e-02,  6.9615e-03, -3.7892e-02,\n",
       "         -7.4856e-02,  9.6442e-03, -1.6967e-03],\n",
       "        [-5.7696e-02, -3.1630e-02,  4.4159e-02,  8.6226e-03, -3.8398e-02,\n",
       "         -7.6725e-02,  8.1578e-03, -2.0582e-03],\n",
       "        [-6.0576e-02, -3.1568e-02,  4.3070e-02,  8.4509e-03, -3.7160e-02,\n",
       "         -7.5377e-02,  1.2421e-02, -2.8248e-04],\n",
       "        [-5.7037e-02, -3.0055e-02,  4.2094e-02,  1.0122e-02, -3.9308e-02,\n",
       "         -7.5453e-02,  1.0740e-02, -8.6974e-04],\n",
       "        [-5.8067e-02, -3.1226e-02,  4.4638e-02,  7.8957e-03, -3.7550e-02,\n",
       "         -7.4133e-02,  9.3185e-03, -9.4450e-04],\n",
       "        [-5.9575e-02, -3.0863e-02,  4.2800e-02,  7.3491e-03, -3.9228e-02,\n",
       "         -7.4562e-02,  8.4115e-03, -1.1520e-03],\n",
       "        [-5.9502e-02, -3.0494e-02,  4.3138e-02,  8.9939e-03, -3.8822e-02,\n",
       "         -7.3495e-02,  1.0606e-02, -1.0087e-03],\n",
       "        [-5.7063e-02, -3.3079e-02,  4.4465e-02,  8.9474e-03, -3.9783e-02,\n",
       "         -7.4107e-02,  1.0471e-02, -2.4146e-03],\n",
       "        [-5.9030e-02, -3.0758e-02,  4.4255e-02,  9.5868e-03, -3.8780e-02,\n",
       "         -7.4932e-02,  1.0728e-02, -2.6854e-03],\n",
       "        [-5.9366e-02, -3.0481e-02,  4.3397e-02,  8.1509e-03, -3.8366e-02,\n",
       "         -7.5475e-02,  1.0956e-02, -2.1235e-03],\n",
       "        [-5.8947e-02, -3.2020e-02,  4.2326e-02,  8.6615e-03, -3.8953e-02,\n",
       "         -7.5028e-02,  8.3606e-03, -1.1742e-05],\n",
       "        [-5.8914e-02, -3.1747e-02,  4.3700e-02,  7.4519e-03, -3.8195e-02,\n",
       "         -7.4996e-02,  1.1625e-02, -1.3730e-03],\n",
       "        [-5.6710e-02, -3.1982e-02,  4.3114e-02,  8.1716e-03, -3.8493e-02,\n",
       "         -7.4063e-02,  9.9588e-03, -3.4483e-03],\n",
       "        [-5.7505e-02, -3.1882e-02,  4.2958e-02,  8.5183e-03, -3.9433e-02,\n",
       "         -7.6151e-02,  9.4769e-03,  7.0988e-04],\n",
       "        [-5.6602e-02, -3.1842e-02,  4.3250e-02,  9.2330e-03, -3.9516e-02,\n",
       "         -7.3978e-02,  6.7721e-03, -3.6698e-03],\n",
       "        [-5.7859e-02, -3.1424e-02,  4.2603e-02,  8.2246e-03, -3.9333e-02,\n",
       "         -7.4016e-02,  1.0762e-02, -1.5207e-03],\n",
       "        [-6.0494e-02, -3.1420e-02,  4.3997e-02,  6.8493e-03, -3.8888e-02,\n",
       "         -7.5001e-02,  9.8038e-03, -1.5797e-03],\n",
       "        [-5.9060e-02, -3.2186e-02,  4.4238e-02,  9.2061e-03, -3.7560e-02,\n",
       "         -7.4394e-02,  7.4981e-03, -2.3050e-03],\n",
       "        [-5.7099e-02, -3.1573e-02,  4.2546e-02,  9.7101e-03, -4.0415e-02,\n",
       "         -7.7222e-02,  1.0128e-02, -2.1369e-03],\n",
       "        [-5.9924e-02, -3.3056e-02,  4.4407e-02,  8.0985e-03, -3.7832e-02,\n",
       "         -7.5827e-02,  1.1295e-02, -1.0208e-03],\n",
       "        [-5.8390e-02, -3.0279e-02,  4.3270e-02,  9.8266e-03, -3.9489e-02,\n",
       "         -7.6977e-02,  1.1889e-02,  2.5657e-04],\n",
       "        [-5.7202e-02, -3.3102e-02,  4.4676e-02,  8.1227e-03, -3.8852e-02,\n",
       "         -7.5393e-02,  8.5114e-03, -3.9023e-03],\n",
       "        [-5.7604e-02, -3.2380e-02,  4.3308e-02,  8.1933e-03, -3.9647e-02,\n",
       "         -7.2598e-02,  1.1447e-02, -1.6327e-03],\n",
       "        [-5.8492e-02, -3.2743e-02,  4.2443e-02,  7.1593e-03, -3.8520e-02,\n",
       "         -7.3625e-02,  8.2484e-03, -2.5178e-03],\n",
       "        [-5.8649e-02, -3.1167e-02,  4.4386e-02,  1.0490e-02, -3.7535e-02,\n",
       "         -7.5677e-02,  1.0351e-02, -1.8087e-03],\n",
       "        [-5.9701e-02, -3.1047e-02,  4.4981e-02,  7.7714e-03, -3.7178e-02,\n",
       "         -7.6388e-02,  1.1468e-02, -1.2406e-03],\n",
       "        [-5.8481e-02, -3.1989e-02,  4.5244e-02,  7.5848e-03, -3.7624e-02,\n",
       "         -7.5423e-02,  9.1399e-03, -8.0278e-04],\n",
       "        [-5.8065e-02, -3.2368e-02,  4.2381e-02,  9.3596e-03, -3.8754e-02,\n",
       "         -7.6276e-02,  9.2082e-03, -2.6715e-03],\n",
       "        [-6.0097e-02, -3.2128e-02,  4.4580e-02,  5.7801e-03, -3.9091e-02,\n",
       "         -7.5210e-02,  9.8617e-03, -1.3033e-03],\n",
       "        [-5.8014e-02, -3.1968e-02,  4.3942e-02,  8.9688e-03, -3.7744e-02,\n",
       "         -7.6380e-02,  1.0341e-02, -2.2727e-03],\n",
       "        [-5.9821e-02, -3.0323e-02,  4.3970e-02,  7.1910e-03, -3.8740e-02,\n",
       "         -7.6779e-02,  8.9187e-03, -1.0853e-03],\n",
       "        [-5.9131e-02, -2.9964e-02,  4.3383e-02,  9.1310e-03, -3.9048e-02,\n",
       "         -7.5771e-02,  9.4177e-03, -3.2599e-03],\n",
       "        [-5.8908e-02, -2.9823e-02,  4.3720e-02,  1.0406e-02, -3.9325e-02,\n",
       "         -7.6255e-02,  1.1033e-02, -9.0300e-04],\n",
       "        [-5.7666e-02, -3.1037e-02,  4.2322e-02,  7.4425e-03, -3.9785e-02,\n",
       "         -7.2902e-02,  1.1195e-02, -1.2488e-03],\n",
       "        [-5.6673e-02, -3.2225e-02,  4.4391e-02,  6.0421e-03, -3.9241e-02,\n",
       "         -7.4860e-02,  8.7476e-03, -2.6323e-03],\n",
       "        [-5.7862e-02, -2.9918e-02,  4.3716e-02,  7.7389e-03, -3.7915e-02,\n",
       "         -7.3462e-02,  1.2050e-02, -2.0912e-03],\n",
       "        [-5.7338e-02, -3.0558e-02,  4.3589e-02,  7.8461e-03, -3.9245e-02,\n",
       "         -7.5158e-02,  9.6020e-03, -3.3238e-03],\n",
       "        [-5.7335e-02, -3.3095e-02,  4.3810e-02,  6.8515e-03, -3.9217e-02,\n",
       "         -7.3908e-02,  1.1194e-02, -3.6102e-03],\n",
       "        [-5.7643e-02, -3.1391e-02,  4.3394e-02,  8.7761e-03, -3.8933e-02,\n",
       "         -7.5224e-02,  1.0739e-02, -1.9112e-03],\n",
       "        [-5.9389e-02, -3.0660e-02,  4.3721e-02,  9.7319e-03, -3.9540e-02,\n",
       "         -7.5355e-02,  1.0227e-02, -3.8612e-03],\n",
       "        [-5.7645e-02, -3.2513e-02,  4.1878e-02,  9.2554e-03, -4.1244e-02,\n",
       "         -7.6720e-02,  1.1803e-02, -2.8445e-04],\n",
       "        [-5.9499e-02, -3.1064e-02,  4.2166e-02,  9.4159e-03, -3.8740e-02,\n",
       "         -7.3217e-02,  1.2484e-02, -1.6373e-03],\n",
       "        [-5.9184e-02, -3.1507e-02,  4.2764e-02,  9.4815e-03, -3.9049e-02,\n",
       "         -7.5177e-02,  1.2204e-02, -8.9011e-04],\n",
       "        [-5.8006e-02, -3.0777e-02,  4.3651e-02,  7.8101e-03, -3.8872e-02,\n",
       "         -7.6949e-02,  8.9929e-03, -1.5595e-03],\n",
       "        [-5.9650e-02, -3.2294e-02,  4.4576e-02,  8.6232e-03, -3.8345e-02,\n",
       "         -7.4262e-02,  7.9892e-03, -2.7636e-03],\n",
       "        [-5.8359e-02, -3.0832e-02,  4.3535e-02,  8.8368e-03, -3.9703e-02,\n",
       "         -7.3383e-02,  1.0231e-02, -2.1264e-03],\n",
       "        [-5.9085e-02, -2.9054e-02,  4.5118e-02,  8.9201e-03, -3.8586e-02,\n",
       "         -7.4821e-02,  1.0027e-02, -1.4767e-03],\n",
       "        [-5.9725e-02, -3.1393e-02,  4.4096e-02,  9.3107e-03, -3.7429e-02,\n",
       "         -7.6251e-02,  8.9742e-03, -1.8244e-03],\n",
       "        [-5.7490e-02, -3.2159e-02,  4.2561e-02,  9.5259e-03, -3.9149e-02,\n",
       "         -7.4452e-02,  1.1546e-02, -1.0999e-03],\n",
       "        [-5.7246e-02, -3.2160e-02,  4.3892e-02,  8.7363e-03, -3.9179e-02,\n",
       "         -7.5365e-02,  1.0438e-02, -1.1632e-03],\n",
       "        [-5.8960e-02, -3.1806e-02,  4.4092e-02,  7.5174e-03, -3.7791e-02,\n",
       "         -7.5927e-02,  8.4480e-03, -2.5836e-03],\n",
       "        [-5.8640e-02, -3.0518e-02,  4.2854e-02,  8.4392e-03, -3.9660e-02,\n",
       "         -7.4868e-02,  1.1985e-02, -7.0340e-04],\n",
       "        [-5.9093e-02, -3.1637e-02,  4.3263e-02,  8.7688e-03, -3.9815e-02,\n",
       "         -7.4013e-02,  1.1784e-02, -2.9643e-03],\n",
       "        [-5.8171e-02, -3.1660e-02,  4.2923e-02,  8.7150e-03, -3.9314e-02,\n",
       "         -7.4650e-02,  1.1838e-02, -3.8979e-04],\n",
       "        [-6.0235e-02, -3.2547e-02,  4.4689e-02,  9.6347e-03, -3.7196e-02,\n",
       "         -7.6498e-02,  9.9689e-03, -2.3327e-03],\n",
       "        [-5.9021e-02, -3.1253e-02,  4.2853e-02,  8.5435e-03, -3.8468e-02,\n",
       "         -7.5568e-02,  1.0155e-02, -1.6255e-03],\n",
       "        [-5.7500e-02, -3.1060e-02,  4.2774e-02,  8.5596e-03, -4.0717e-02,\n",
       "         -7.4716e-02,  1.1258e-02, -1.2207e-03],\n",
       "        [-6.0496e-02, -3.0983e-02,  4.2395e-02,  1.1019e-02, -3.8907e-02,\n",
       "         -7.5678e-02,  8.9874e-03, -1.0577e-03],\n",
       "        [-5.9006e-02, -3.3948e-02,  4.2153e-02,  8.6555e-03, -3.7884e-02,\n",
       "         -7.3077e-02,  8.0836e-03, -2.0527e-03],\n",
       "        [-5.8645e-02, -3.1619e-02,  4.4134e-02,  1.0864e-02, -3.8930e-02,\n",
       "         -7.5231e-02,  1.0643e-02, -1.9815e-04],\n",
       "        [-5.8967e-02, -3.0369e-02,  4.4768e-02,  7.7877e-03, -3.8382e-02,\n",
       "         -7.4711e-02,  1.0760e-02, -1.6459e-03],\n",
       "        [-5.7813e-02, -3.1310e-02,  4.4447e-02,  7.2797e-03, -3.8996e-02,\n",
       "         -7.6600e-02,  9.7534e-03, -3.2811e-03],\n",
       "        [-6.1048e-02, -3.0070e-02,  4.4019e-02,  7.3198e-03, -3.7895e-02,\n",
       "         -7.4762e-02,  1.0822e-02, -1.8269e-03]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class BC(nn.Module):\n",
    "    def __init__(self, obs_dim=55, act_dim=8, hidden_size=256, num_layers=4, num_heads=8, k=10):\n",
    "        super(BC, self).__init__()\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.k = k\n",
    "        self.positional_encoding = PositionalEncoding(d_model=hidden_size)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc_in = nn.Linear(obs_dim * k, hidden_size)  # To match the Transformer input dimension\n",
    "        self.fc_out = nn.Linear(hidden_size, act_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the last k observations\n",
    "        x = x.view(-1, self.k * self.obs_dim)\n",
    "\n",
    "        # Pass through a fully connected layer to get the right dimension\n",
    "        x = self.fc_in(x)\n",
    "        x = x.unsqueeze(1)  # Add a dummy sequence length dimension expected by Transformer\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Transformer expects a sequence, but we're handling individual observations\n",
    "        # So we treat each batch of k observations as a 'sequence' of length 1\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "\n",
    "        # Since we have a 'sequence' of length 1, we just select that one output\n",
    "        transformer_output = transformer_output.squeeze(1)\n",
    "\n",
    "        # And finally, pass it through the output fully connected layer\n",
    "        action = self.fc_out(transformer_output)\n",
    "\n",
    "        return action\n",
    "\n",
    "# Initialize the model\n",
    "bc_transformer = BCTransformer()\n",
    "\n",
    "# Create a dummy input tensor of the correct shape\n",
    "# Assuming input is (batch_size, k, obs_dim), where k is the number of past observations considered\n",
    "dummy_input = torch.randn(100, 10, 55)  # 100 examples, each with 10 past observations\n",
    "\n",
    "# Forward pass\n",
    "output = bc_transformer(dummy_input)\n",
    "print(output.shape)  # Should print torch.Size([100, 8]), assuming act_dim is the output dimension\n",
    "\n",
    "bc=BC()\n",
    "bc(torch.randn(100, 20, 55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr: float = 2e-4,\n",
    "          weight_decay: float = 2e-6,\n",
    "          batch_size: int = 256,\n",
    "          seq_len: int = 8,\n",
    "          epochs: int = 100,\n",
    "          seed: int = 42,\n",
    "          log_freq: int = 5):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = StackDatasetOriginalSequential(seq_len=seq_len, train=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = BC().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    train_epoch_idx = []\n",
    "    train_losses = []\n",
    "    validation_epoch_idx = []\n",
    "    validation_losses = []\n",
    "    best_ckpt = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_path)\n",
    "    writer.add_graph(model, torch.zeros(1, seq_len, 55).to(device))\n",
    "\n",
    "    # summary(model, (SEQ_LEN, 55))\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        if epoch % log_freq == 0:\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(ckpt_path, f'bc_{epoch}.pt'))\n",
    "            validation_loss = validate(model, seq_len)\n",
    "            validation_epoch_idx.append(epoch)\n",
    "            validation_losses.append(validation_loss)\n",
    "            writer.add_scalar('Loss/Validation', validation_loss, epoch)\n",
    "            model.train()\n",
    "            if validation_loss < best_loss:\n",
    "                best_loss = validation_loss\n",
    "                best_ckpt = os.path.join(ckpt_path, f'bc_{epoch}.pt')\n",
    "\n",
    "        for obs, action in dataloader:\n",
    "            obs = obs.to(device)\n",
    "            action = action.to(device)\n",
    "\n",
    "            pred = model(obs)\n",
    "            train_loss = criterion(pred, action)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_epoch_idx.append(epoch + 1)\n",
    "        train_losses.append(train_loss.item())\n",
    "        writer.add_scalar('Loss/Train', train_loss.item(), epoch + 1)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(ckpt_path, f'bc_{epoch+1}.pt'))\n",
    "    validation_loss = validate(model, seq_len)\n",
    "    validation_epoch_idx.append(epoch+1)\n",
    "    validation_losses.append(validation_loss)\n",
    "    writer.add_scalar('Loss/Validation', validation_loss, epoch+1)\n",
    "    if validation_loss < best_loss:\n",
    "        best_loss = validation_loss\n",
    "        best_ckpt = os.path.join(ckpt_path, f'bc_{epoch+1}.pt')\n",
    "\n",
    "    log = dict(train_epochs=train_epoch_idx,\n",
    "               validation_epochs=validation_epoch_idx,\n",
    "               train_losses=train_losses,\n",
    "               validation_losses=validation_losses,\n",
    "               best_ckpt=best_ckpt,\n",
    "               best_loss=best_loss,\n",
    "               lr=lr,\n",
    "               weight_decay=weight_decay,\n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               seed=seed,\n",
    "               log_freq=log_freq)\n",
    "\n",
    "    with open(os.path.join(log_path, 'train_log.json'), 'w') as f:\n",
    "        json.dump(log, f, indent=4)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    return best_ckpt\n",
    "\n",
    "\n",
    "def validate(model: BC, seq_len: int):\n",
    "    model.eval()\n",
    "    dataset = StackDatasetOriginalSequential(seq_len=seq_len, train=False)\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for obs, action in dataloader:\n",
    "            obs = obs.to(device)\n",
    "            action = action.to(device)\n",
    "\n",
    "            pred = model(obs)\n",
    "            loss = criterion(pred, action)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return np.sum(losses) / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ckpt: str,\n",
    "         seq_len: int,\n",
    "         max_steps: int = 300,\n",
    "         num_episodes: int = 100):\n",
    "\n",
    "    env = gym.make('StackCube-v0',\n",
    "                   obs_mode=\"state_dict\",\n",
    "                   control_mode=\"pd_joint_delta_pos\",\n",
    "                   max_episode_steps=max_steps)\n",
    "\n",
    "    model = BC()\n",
    "    model.load_state_dict(torch.load(ckpt))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    best_return = -np.inf\n",
    "    best_seed = None\n",
    "    returns = {}\n",
    "    success = 0\n",
    "    writer = SummaryWriter(tensorboard_path)\n",
    "\n",
    "    for seed in tqdm(range(num_episodes)):\n",
    "        obs, _ = env.reset(seed=seed)\n",
    "        obs = flatten_obs(obs)\n",
    "        buffer = init_deque(obs, seq_len)\n",
    "        sequence = np.array(buffer)\n",
    "        G = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        with torch.no_grad():\n",
    "            while not terminated and not truncated:\n",
    "                sequence = torch.from_numpy(sequence[None]).to(device)\n",
    "                action = model(sequence)\n",
    "                action = action.detach().cpu().numpy()\n",
    "                obs, reward, terminated, truncated, info = env.step(action[0])\n",
    "                obs = flatten_obs(obs)\n",
    "                sequence = update_deque(obs=obs, window=buffer)\n",
    "                G += reward\n",
    "\n",
    "        if G > best_return:\n",
    "            best_return = G\n",
    "            best_seed = seed\n",
    "\n",
    "        if info['success']:\n",
    "            success += 1\n",
    "\n",
    "        returns[seed] = G\n",
    "        writer.add_scalar('Return', G, seed)\n",
    "    env.close()\n",
    "\n",
    "    log = dict(returns=returns,\n",
    "               best_seed=best_seed,\n",
    "               best_return=best_return,\n",
    "               max_steps=max_steps,\n",
    "               num_episodes=num_episodes,\n",
    "               success_rate = success / num_episodes)\n",
    "\n",
    "    with open(os.path.join(log_path, 'test_log.json'), 'w') as f:\n",
    "        json.dump(log, f, indent=4)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    return best_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_video(ckpt: str,\n",
    "                 seq_len: int,\n",
    "                 seed: int,\n",
    "                 max_steps: int = 300):\n",
    "    \n",
    "    env = gym.make('StackCube-v0',\n",
    "                render_mode=\"cameras\",\n",
    "                enable_shadow=True,\n",
    "                obs_mode=\"state_dict\",\n",
    "                control_mode=\"pd_joint_delta_pos\", \n",
    "                max_episode_steps=max_steps)\n",
    "\n",
    "    env = RecordEpisode(\n",
    "        env,\n",
    "        log_path,\n",
    "        info_on_video=True,\n",
    "        save_trajectory=False\n",
    "    )\n",
    "\n",
    "\n",
    "    model = BC()\n",
    "    model.load_state_dict(torch.load(ckpt))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    obs = flatten_obs(obs)\n",
    "    buffer = init_deque(obs, seq_len)\n",
    "    sequence = np.array(buffer)\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while not terminated and not truncated:\n",
    "            sequence = torch.from_numpy(sequence[None]).to(device)\n",
    "            action = model(sequence)\n",
    "            action = action.detach().cpu().numpy()\n",
    "            obs, reward, terminated, truncated, info = env.step(action[0])\n",
    "            obs = flatten_obs(obs)\n",
    "            sequence = update_deque(obs=obs, window=buffer)\n",
    "\n",
    "    env.flush_video(suffix=f'BC_{seed}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 149/150 [21:26<00:08,  8.08s/it]"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 20\n",
    "print('Training...')\n",
    "best_ckpt = train(seq_len=SEQ_LEN, epochs=150)\n",
    "print('Testing...')\n",
    "best_seed = test(ckpt=best_ckpt, seq_len=SEQ_LEN)\n",
    "print('Rendering...')\n",
    "render_video(ckpt=best_ckpt, seq_len=SEQ_LEN, seed=best_seed, max_steps=500)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class BCTransformer(nn.Module):\n",
    "    def __init__(self, obs_dim=55, act_dim=8, hidden_size=256, num_layers=4, num_heads=8, k=10):\n",
    "        super(BCTransformer, self).__init__()\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.k = k\n",
    "        self.positional_encoding = PositionalEncoding(d_model=hidden_size)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc_in = nn.Linear(obs_dim * k, hidden_size)  # To match the Transformer input dimension\n",
    "        self.fc_out = nn.Linear(hidden_size, act_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the last k observations\n",
    "        x = x.view(-1, self.k * self.obs_dim)\n",
    "\n",
    "        # Pass through a fully connected layer to get the right dimension\n",
    "        x = self.fc_in(x)\n",
    "        x = x.unsqueeze(1)  # Add a dummy sequence length dimension expected by Transformer\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Transformer expects a sequence, but we're handling individual observations\n",
    "        # So we treat each batch of k observations as a 'sequence' of length 1\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "\n",
    "        # Since we have a 'sequence' of length 1, we just select that one output\n",
    "        transformer_output = transformer_output.squeeze(1)\n",
    "\n",
    "        # And finally, pass it through the output fully connected layer\n",
    "        action = self.fc_out(transformer_output)\n",
    "\n",
    "        return action\n",
    "\n",
    "# Initialize the model\n",
    "bc_transformer = BCTransformer()\n",
    "\n",
    "# Create a dummy input tensor of the correct shape\n",
    "# Assuming input is (batch_size, k, obs_dim), where k is the number of past observations considered\n",
    "dummy_input = torch.randn(100, 10, 55)  # 100 examples, each with 10 past observations\n",
    "\n",
    "# Forward pass\n",
    "output = bc_transformer(dummy_input)\n",
    "print(output.shape)  # Should print torch.Size([100, 8]), assuming act_dim is the output dimension\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
